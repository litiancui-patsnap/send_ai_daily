#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
AI æ—¥æŠ¥è‡ªåŠ¨åŒ–ç³»ç»Ÿ
- ä» RSS æºæŠ“å–æœ€è¿‘ 48 å°æ—¶å†…å®¹
- ä½¿ç”¨å¤§æ¨¡å‹ API è¯„åˆ†å¹¶ç”Ÿæˆæ—¥æŠ¥ï¼ˆæ”¯æŒ OpenAI / é€šä¹‰åƒé—®ï¼‰
- å‘é€åˆ°é£ä¹¦ç¾¤ï¼ˆè‡ªå®šä¹‰æœºå™¨äºº + ç­¾åæ ¡éªŒï¼‰
- åŸºäº sha256(link) å»é‡
"""

import os
import sys
import json
import hashlib
import hmac
import base64
import time
from datetime import datetime, timedelta, timezone
from pathlib import Path
from typing import List, Dict, Optional

import requests
import feedparser
from dateutil import parser as date_parser


# ==================== é…ç½® ====================
# LLM é…ç½®ï¼ˆæ”¯æŒ OpenAI æˆ–é€šä¹‰åƒé—®ï¼‰
LLM_PROVIDER = os.getenv("LLM_PROVIDER", "openai").lower()  # openai æˆ– qwen
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "")
DASHSCOPE_API_KEY = os.getenv("DASHSCOPE_API_KEY", "")
QWEN_MODEL = os.getenv("QWEN_MODEL", "qwen-plus")

# é£ä¹¦é…ç½®
FEISHU_WEBHOOK_URL = os.getenv("FEISHU_WEBHOOK_URL", "")
FEISHU_SECRET = os.getenv("FEISHU_SECRET", "")

# RSS é…ç½®
RSS_URLS_RAW = os.getenv("RSS_URLS", "")

RSS_URLS = [line.strip() for line in RSS_URLS_RAW.strip().split("\n") if line.strip()]
SENT_HASHES_FILE = Path("data/sent_hashes.txt")
MAX_CANDIDATES = 30
TOP_N = 3
HOURS_WINDOW = 48


# ==================== å·¥å…·å‡½æ•° ====================
def load_sent_hashes() -> set:
    """åŠ è½½å·²å‘é€çš„ hash é›†åˆ"""
    if not SENT_HASHES_FILE.exists():
        SENT_HASHES_FILE.parent.mkdir(parents=True, exist_ok=True)
        SENT_HASHES_FILE.touch()
        return set()
    with open(SENT_HASHES_FILE, "r", encoding="utf-8") as f:
        return set(line.strip() for line in f if line.strip())


def save_sent_hashes(hashes: set):
    """ä¿å­˜å·²å‘é€çš„ hash é›†åˆ"""
    SENT_HASHES_FILE.parent.mkdir(parents=True, exist_ok=True)
    with open(SENT_HASHES_FILE, "w", encoding="utf-8") as f:
        for h in sorted(hashes):
            f.write(h + "\n")


def hash_link(link: str) -> str:
    """è®¡ç®—é“¾æ¥çš„ sha256 hash"""
    return hashlib.sha256(link.encode("utf-8")).hexdigest()


def is_recent(published_str: str, hours: int = HOURS_WINDOW) -> bool:
    """åˆ¤æ–­æ¡ç›®æ˜¯å¦åœ¨æœ€è¿‘ N å°æ—¶å†…"""
    try:
        pub_time = date_parser.parse(published_str)
        if pub_time.tzinfo is None:
            pub_time = pub_time.replace(tzinfo=timezone.utc)
        now = datetime.now(timezone.utc)
        return (now - pub_time) <= timedelta(hours=hours)
    except Exception:
        return False


# ==================== RSS æŠ“å– ====================
def fetch_rss_entries() -> List[Dict]:
    """ä»æ‰€æœ‰ RSS æºæŠ“å–æœ€è¿‘ 48 å°æ—¶å†…çš„æ¡ç›®"""
    candidates = []
    sent_hashes = load_sent_hashes()

    for url in RSS_URLS:
        try:
            print(f"[INFO] æŠ“å– RSS: {url}")
            feed = feedparser.parse(url)
            for entry in feed.entries:
                link = entry.get("link", "")
                if not link:
                    continue
                link_hash = hash_link(link)
                if link_hash in sent_hashes:
                    continue
                published = entry.get("published", entry.get("updated", ""))
                if not is_recent(published, HOURS_WINDOW):
                    continue
                title = entry.get("title", "")
                summary = entry.get("summary", entry.get("description", ""))
                if len(summary) > 500:
                    summary = summary[:500] + "..."
                candidates.append({
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "published": published,
                    "hash": link_hash
                })
                if len(candidates) >= MAX_CANDIDATES:
                    break
        except Exception as e:
            print(f"[WARN] æŠ“å– {url} å¤±è´¥: {e}")
            continue
        if len(candidates) >= MAX_CANDIDATES:
            break

    print(f"[INFO] å…±æ”¶é›† {len(candidates)} æ¡å€™é€‰")
    return candidates


# ==================== LLM API è°ƒç”¨ ====================
def call_llm_json(system_prompt: str, user_prompt: str) -> Dict:
    """è°ƒç”¨å¤§æ¨¡å‹ APIï¼Œè¦æ±‚è¿”å› JSONï¼ˆæ”¯æŒ OpenAI / é€šä¹‰åƒé—®ï¼‰"""
    if LLM_PROVIDER == "qwen":
        return call_qwen_json(system_prompt, user_prompt)
    else:
        return call_openai_json(system_prompt, user_prompt)


def call_openai_json(system_prompt: str, user_prompt: str, model: str = "gpt-4o-2024-08-06") -> Dict:
    """è°ƒç”¨ OpenAI APIï¼Œè¦æ±‚è¿”å› JSON"""
    url = "https://api.openai.com/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {OPENAI_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "response_format": {"type": "json_object"}
    }
    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]
        return json.loads(content)
    except Exception as e:
        print(f"[ERROR] OpenAI API è°ƒç”¨å¤±è´¥: {e}")
        sys.exit(1)


def call_qwen_json(system_prompt: str, user_prompt: str) -> Dict:
    """è°ƒç”¨é€šä¹‰åƒé—® APIï¼Œè¦æ±‚è¿”å› JSON"""
    # æ£€æŸ¥å¿…è¦çš„é…ç½®
    if not DASHSCOPE_API_KEY:
        print("[ERROR] æœªé…ç½® DASHSCOPE_API_KEY")
        sys.exit(1)

    model = QWEN_MODEL if QWEN_MODEL else "qwen-plus"
    print(f"[INFO] ä½¿ç”¨é€šä¹‰åƒé—®æ¨¡å‹: {model}")

    url = "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {DASHSCOPE_API_KEY}",
        "Content-Type": "application/json"
    }

    # åˆå¹¶ system å’Œ user prompt
    combined_prompt = f"{system_prompt}\n\n{user_prompt}"

    payload = {
        "model": model,
        "messages": [
            {"role": "user", "content": combined_prompt}
        ],
        "response_format": {"type": "json_object"}
    }

    try:
        print(f"[DEBUG] è¯·æ±‚ URL: {url}")
        print(f"[DEBUG] è¯·æ±‚ model: {payload['model']}")
        resp = requests.post(url, headers=headers, json=payload, timeout=60)
        print(f"[DEBUG] å“åº”çŠ¶æ€ç : {resp.status_code}")
        resp.raise_for_status()
        data = resp.json()
        content = data["choices"][0]["message"]["content"]
        return json.loads(content)
    except requests.exceptions.HTTPError as e:
        print(f"[ERROR] é€šä¹‰åƒé—® API HTTP é”™è¯¯: {e}")
        print(f"[DEBUG] å“åº”å†…å®¹: {resp.text}")
        sys.exit(1)
    except Exception as e:
        print(f"[ERROR] é€šä¹‰åƒé—® API è°ƒç”¨å¤±è´¥: {e}")
        print(f"[DEBUG] å“åº”å†…å®¹: {resp.text if 'resp' in locals() else 'No response'}")
        sys.exit(1)


# ==================== è¯„åˆ†é˜¶æ®µ ====================
def score_entries(entries: List[Dict]) -> List[Dict]:
    """ä½¿ç”¨ LLM å¯¹æ¡ç›®æ‰“åˆ†ï¼Œè¿”å› Top N"""
    if not entries:
        return []

    system_prompt = """ä½ æ˜¯ä¸€åèµ„æ·± AI å·¥ç¨‹å¸ˆå’ŒæŠ€æœ¯ç¼–è¾‘ã€‚
ä½ çš„ä»»åŠ¡æ˜¯ä»ä¸€æ‰¹ RSS æ¡ç›®ä¸­ï¼Œç­›é€‰å‡ºæœ€å€¼å¾—ä¼ä¸šå†…éƒ¨ AI å›¢é˜Ÿå…³æ³¨çš„å†…å®¹ã€‚
è¯„åˆ†æ ‡å‡†ï¼ˆ0~10ï¼‰ï¼š
- å¤§æ¨¡å‹ / AI å¹³å°èƒ½åŠ›æ›´æ–°ï¼š9~10
- Agent / Tool / RAG / ç³»ç»Ÿè®¾è®¡å®è·µï¼š7~9
- äº§å“åº”ç”¨æ¡ˆä¾‹ã€è¯„æµ‹ï¼š5~7
- æ³›æ³›è€Œè°ˆã€è¥é”€è½¯æ–‡ï¼š0~3

è¯·è¿”å› JSON æ•°ç»„ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼šlink, score, reasonã€‚
åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    user_prompt = f"""è¯·å¯¹ä»¥ä¸‹ {len(entries)} æ¡ RSS æ¡ç›®æ‰“åˆ†ï¼š

{json.dumps(entries, ensure_ascii=False, indent=2)}

è¿”å›æ ¼å¼ï¼š
{{
  "scores": [
    {{"link": "...", "score": 8.5, "reason": "..."}},
    ...
  ]
}}"""

    result = call_llm_json(system_prompt, user_prompt)
    scores = result.get("scores", [])

    # æŒ‰ score æ’åºï¼Œå– Top N
    scores.sort(key=lambda x: x.get("score", 0), reverse=True)
    top_scores = scores[:TOP_N]

    # è¡¥å……å®Œæ•´æ¡ç›®ä¿¡æ¯
    link_map = {e["link"]: e for e in entries}
    top_entries = []
    for s in top_scores:
        link = s["link"]
        if link in link_map:
            entry = link_map[link].copy()
            entry["score"] = s["score"]
            entry["score_reason"] = s["reason"]
            top_entries.append(entry)

    print(f"[INFO] è¯„åˆ†å®Œæˆï¼ŒTop {TOP_N}: {len(top_entries)} æ¡")
    return top_entries


# ==================== æ—¥æŠ¥ç”Ÿæˆé˜¶æ®µ ====================
def generate_daily_report(top_entries: List[Dict]) -> Dict:
    """ä½¿ç”¨ LLM ç”Ÿæˆæ—¥æŠ¥å†…å®¹"""
    system_prompt = """ä½ æ˜¯ä¸€åä¼ä¸šå†…éƒ¨ AI æ—¥æŠ¥ç¼–è¾‘ï¼Œè´Ÿè´£å°†æŠ€æœ¯åŠ¨æ€è½¬åŒ–ä¸ºå¯¹ä¸åŒè§’è‰²çš„å®ç”¨æ´å¯Ÿã€‚

ä½ éœ€è¦è¾“å‡ºä¸¥æ ¼ JSONï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- headline: ä¸€å¥è¯æ ‡é¢˜ï¼ˆ20å­—å†…ï¼‰
- changes: æŠ€æœ¯å˜åŒ–è¦ç‚¹ï¼ˆæ•°ç»„ï¼Œ2~3æ¡ï¼‰
- impacts: å¯¹ 10 ä¸ªè§’è‰²çš„å½±å“ï¼ˆæ¯ä¸ªè§’è‰² 1 å¥è¯ï¼‰
  - boss: è€æ¿
  - market: å¸‚åœº
  - pm: äº§å“ç»ç†
  - presales: å”®å‰
  - algo: ç®—æ³•å·¥ç¨‹å¸ˆ
  - frontend: å‰ç«¯å·¥ç¨‹å¸ˆ
  - backend: åç«¯å·¥ç¨‹å¸ˆ
  - ui: UIè®¾è®¡å¸ˆ
  - qa: æµ‹è¯•å·¥ç¨‹å¸ˆ
  - surveying: é¡¹ç›®ç»ç†
- action: å»ºè®®åŠ¨ä½œï¼ˆæšä¸¾ï¼šğŸ§ªè¯•ç‚¹ / ğŸ‘€è§‚å¯Ÿ / âŒå¿½ç•¥ï¼‰
- action_detail: åŠ¨ä½œç»†èŠ‚ï¼ˆ1~2å¥è¯ï¼‰
- sources: æ¥æºæ•°ç»„ [{title, link}]

åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–æ–‡å­—ã€‚"""

    user_prompt = f"""è¯·åŸºäºä»¥ä¸‹ {len(top_entries)} æ¡å†…å®¹ç”Ÿæˆä»Šæ—¥ AI æ—¥æŠ¥ï¼š

{json.dumps(top_entries, ensure_ascii=False, indent=2)}"""

    return call_llm_json(system_prompt, user_prompt)


# ==================== é£ä¹¦æ¨é€ ====================
def send_to_feishu(report: Dict):
    """å‘é€æ—¥æŠ¥åˆ°é£ä¹¦ç¾¤ï¼ˆå¸¦ç­¾åæ ¡éªŒï¼‰"""
    if not FEISHU_WEBHOOK_URL:
        print("[WARN] æœªé…ç½® FEISHU_WEBHOOK_URLï¼Œè·³è¿‡å‘é€")
        return

    # ç­¾å
    timestamp = str(int(time.time()))
    sign = ""
    if FEISHU_SECRET:
        string_to_sign = f"{timestamp}\n{FEISHU_SECRET}"
        hmac_code = hmac.new(
            string_to_sign.encode("utf-8"),
            digestmod=hashlib.sha256
        ).digest()
        sign = base64.b64encode(hmac_code).decode("utf-8")

    # æ„å»º impacts åˆ—è¡¨
    impacts_text = ""
    for role, desc in report["impacts"].items():
        role_name = {
            "boss": "è€æ¿",
            "market": "å¸‚åœº",
            "pm": "äº§å“ç»ç†",
            "presales": "å”®å‰",
            "algo": "ç®—æ³•å·¥ç¨‹å¸ˆ",
            "frontend": "å‰ç«¯å·¥ç¨‹å¸ˆ",
            "backend": "åç«¯å·¥ç¨‹å¸ˆ",
            "ui": "UIè®¾è®¡å¸ˆ",
            "qa": "æµ‹è¯•å·¥ç¨‹å¸ˆ",
            "surveying": "é¡¹ç›®ç»ç†"
        }.get(role, role)
        impacts_text += f"**{role_name}**: {desc}\n"

    # æ„å»º sources åˆ—è¡¨
    sources_text = ""
    for idx, src in enumerate(report["sources"], 1):
        sources_text += f"{idx}. [{src['title']}]({src['link']})\n"

    # æ„å»º changes åˆ—è¡¨
    changes_text = "\n".join([f"â€¢ {c}" for c in report["changes"]])

    card_content = {
        "config": {"wide_screen_mode": True},
        "header": {
            "title": {"tag": "plain_text", "content": f"ğŸ“° AI æ—¥æŠ¥ | {datetime.now().strftime('%Y-%m-%d')}"},
            "template": "blue"
        },
        "elements": [
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**{report['headline']}**"}
            },
            {"tag": "hr"},
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**ğŸ“Œ æŠ€æœ¯å˜åŒ–**\n{changes_text}"}
            },
            {"tag": "hr"},
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**ğŸ‘¥ è§’è‰²å½±å“**\n{impacts_text}"}
            },
            {"tag": "hr"},
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**ğŸ¯ å»ºè®®åŠ¨ä½œ**: {report['action']}\n{report['action_detail']}"}
            },
            {"tag": "hr"},
            {
                "tag": "div",
                "text": {"tag": "lark_md", "content": f"**ğŸ“š æ¥æº**\n{sources_text}"}
            }
        ]
    }

    payload = {
        "timestamp": timestamp,
        "sign": sign,
        "msg_type": "interactive",
        "card": card_content
    }

    # é‡è¯•æœºåˆ¶
    for attempt in range(3):
        try:
            resp = requests.post(FEISHU_WEBHOOK_URL, json=payload, timeout=10)
            resp.raise_for_status()
            result = resp.json()
            if result.get("code") == 0:
                print("[INFO] é£ä¹¦æ¨é€æˆåŠŸ")
                return
            else:
                print(f"[WARN] é£ä¹¦æ¨é€å¤±è´¥: {result}")
        except Exception as e:
            print(f"[WARN] é£ä¹¦æ¨é€å¼‚å¸¸ (attempt {attempt+1}): {e}")
            if attempt < 2:
                time.sleep(2 ** attempt)
    print("[ERROR] é£ä¹¦æ¨é€æœ€ç»ˆå¤±è´¥")


# ==================== ä¸»æµç¨‹ ====================
def main():
    print(f"[INFO] å¼€å§‹æ‰§è¡Œ AI æ—¥æŠ¥ä»»åŠ¡ - {datetime.now().isoformat()}")

    # 1. æŠ“å– RSS
    candidates = fetch_rss_entries()
    if not candidates:
        print("[INFO] æ— æ–°å†…å®¹ï¼Œé€€å‡º")
        return

    # 2. è¯„åˆ†
    top_entries = score_entries(candidates)
    if not top_entries:
        print("[INFO] æ— é«˜åˆ†å†…å®¹ï¼Œé€€å‡º")
        return

    # 3. ç”Ÿæˆæ—¥æŠ¥
    report = generate_daily_report(top_entries)
    print("[INFO] æ—¥æŠ¥ç”Ÿæˆå®Œæˆ")
    print(json.dumps(report, ensure_ascii=False, indent=2))

    # 4. å‘é€é£ä¹¦
    send_to_feishu(report)

    # 5. æ›´æ–°å»é‡æ–‡ä»¶
    sent_hashes = load_sent_hashes()
    new_hashes = {e["hash"] for e in top_entries}
    sent_hashes.update(new_hashes)
    save_sent_hashes(sent_hashes)
    print(f"[INFO] å·²æ›´æ–°å»é‡æ–‡ä»¶ï¼Œæ–°å¢ {len(new_hashes)} æ¡")


if __name__ == "__main__":
    main()
